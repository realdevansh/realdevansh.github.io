  ---
  layout: post
  title: OEM Vulnerability Scrapper
  ---

The problem statement wants the solution to scrape OEM's vendor feeds, blogs/forums and other responsible platforms for information regarding new vulnerabilities. Which would be fed into a database, the newly found vulnerability would be reported to the responsible personnel via email, the format would be same as provided in the problem statement.

The approach to the scraper should be dynamic so as to reduce overheads when dealing with redundant data when scraping websites. Also, use authorized public api's dynamically so as to reduce the said overhead. This approach would not only reduce computation cost but significantly improve the concurrency of database. Our approach focuses on scraping multiple websites for vulnerability information regarding a particular software. We plan to make the tool such that it can be deployed to a server that has privileged access to multiple servers of an organisation so that it can get accurate version for all the dependencies and build the list of websites it plans to scrape for data. 

This list can be built by a crawler, crawling for webpages that contains certain information regarding new vulnerabilities. Then if the public api is easily available for free of cost then it would get the data via json web toke(jtw), else the 'spider' would scrape for vulnerability data on the websites, looking for table and matching strings by regular expressions to get the target information. The decision of pushing the data in the database would be made if data is not present in it already. There should be 2 databases as well, one that contains latest vulnerabilities and the other would only maintain all the filled out vulnerabilities. The second database would be much bigger and would not be accessible to scraper directly, the first database would contain all the active vulnerabilities, even those which don't have a mitigation strategy or CVE id. Once a vulnerability gets patched by the vendor it'd be pushed into the second database. The first will be sorted by simple hash table to quickly find the exact vulnerability information one might be interested in. The Second database might be sorted according to published time to keep things clean. The latest vulnerability data will be emailed from the first database and the second database might be looked upon for reference.

Our addition to the solution is adding a dashboard with integrated llm which can give better insights into mitigation strategies and even link some Common Weakness Enumeration(CWE) to the discovered vulnerability which might help the person using dashboard better mitigate the threats. The dashboard also gives insight from second database which might be helpful for the organisation by changing the weak/soft points in their IT/OT infrastructure. This is our planned approach to solve the problem. We also have a prototype which is under development.

Our prototype's plan is to scrape a list of websites provided on cve-mitre's website under the webpage, "List of CNA's", we firstly scrape to get information regarding CNA's that might also be OEM's. Then we crawl to find the vulnerability list. We then scrape them for vulnerability information and put it in a csv file which is then used by the dashboard to automatically send email to the listed email accounts. In addition, our prototype also uses free gemini-api to give insights about the report. It also generates a detailed overview of data present in the csv database. 

We plan on making this an open-source tool for security researchers who might need more instantaneous solutions then NVD and might need a quicker way then individually looking on multiple vendor feeds for latest vulnerabilities. This tool could be useful for everyone and not only critical sector organisations such as Energy, Emergency Service, Airlines, etc.

We hope on making this model as big as fischerfang model used by deutsche-telekom.
